{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b264bc-53ce-4889-acb6-ec561cc4fbc6",
   "metadata": {
    "gather": {
     "logged": 1682233468825
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "secret_subscription_id_value = ''\n",
    "resource_group = ''\n",
    "workspace_name = ''\n",
    "workspace = Workspace(secret_subscription_id_value, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce4cffd-5791-4f47-a806-9ad019f14c55",
   "metadata": {
    "gather": {
     "logged": 1682234175162
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/yellow_tripdata_2015-02.csv', engine='python')\n",
    "df = df.sample(frac=0.2)\n",
    "df_backup = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a853f06-db84-4a95-b287-80b0ecac235b",
   "metadata": {
    "gather": {
     "logged": 1682234187265
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Change Data type \n",
    "df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']] = df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']].apply(pd.to_datetime)\n",
    "df[['VendorID','RateCodeID','payment_type', 'store_and_fwd_flag' ]] = df[['VendorID','RateCodeID','payment_type', 'store_and_fwd_flag' ]].astype(str)\n",
    "\n",
    "df = df.drop_duplicates(keep='last')  # drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77555c1-548a-4643-9a5a-e6904b608300",
   "metadata": {
    "gather": {
     "logged": 1682234188995
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2435102, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove pickup and drops outside newyork city limits\n",
    "\n",
    "df.drop(df.index[\n",
    "        ~((df['pickup_latitude'].between(40.496115395170364, 40.91553277700258)) &\n",
    "          (df['pickup_longitude'].between(-74.25559136315209, -73.7000090639354))) \n",
    "], inplace=True)\n",
    "\n",
    "df.drop(df.index[\n",
    "        ~((df['dropoff_latitude'].between(40.496115395170364, 40.91553277700258)) &\n",
    "          (df['dropoff_longitude'].between(-74.25559136315209, -73.7000090639354))) \n",
    "], inplace=True)\n",
    "df.shape\n",
    "\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea36c8c-3580-4236-bfe4-d413d95f8304",
   "metadata": {
    "gather": {
     "logged": 1682234189725
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower limit of dist is -2.1049999999999995 and upper limit of dist is 6.174999999999999\n"
     ]
    }
   ],
   "source": [
    "# Remove the outliers - tip distance 0 to be removed\n",
    "\n",
    "def computer_remove_outliers(df, x, remove_outlier=1):\n",
    "    q1 = df[x].quantile(0.25)\n",
    "    q3 = df[x].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    ll = q1 - 1.5*(iqr)\n",
    "    ul = q3 + 1.5*(iqr)\n",
    "    print('lower limit of dist is {} and upper limit of dist is {}'.format(ll, ul))\n",
    "\n",
    "    # remove all negative distances and distances greater than ul\n",
    "    if remove_outlier==1:\n",
    "        df = df[(df[x]>ll) & (df[x]<ul)]\n",
    "        df.shape\n",
    "        return df\n",
    "\n",
    "df = computer_remove_outliers(df, 'trip_distance')\n",
    "df = df[df['trip_distance'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1128c7-47e3-4bad-915e-c454bb94aad9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Update datatype of datetime columns\n",
    "\n",
    "df['time_diff_hours'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).values.astype('timedelta64[h]')\n",
    "df['time_diff_hours'] = df['time_diff_hours'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0ef441-c87f-4ae0-bcf9-84d36bede92c",
   "metadata": {
    "gather": {
     "logged": 1682234190802
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers in date columns - trips more than 2 hours could be outliers and to be removed\n",
    "\n",
    "df['time_diff_hours'].value_counts().sort_values(ascending=False)\n",
    "df = df[df[\"time_diff_hours\"] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf3fd5-8997-41c5-a605-ac8644cc7993",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# remove cancelled rides\n",
    "\n",
    "df['time_diff_min'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).values.astype('timedelta64[m]').astype(int)\n",
    "df[df['time_diff_min']==0]\n",
    "df = df[df[\"time_diff_min\"] > 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e740b92-f813-4372-bdf9-2261abb76431",
   "metadata": {
    "gather": {
     "logged": 1682234191559
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850959, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove uncommon passenger counts\n",
    "\n",
    "df[\"passenger_count\"].value_counts(normalize=True).sort_values(ascending=False) * 100\n",
    "df = df[(df[\"passenger_count\"] <=2) & (df[\"passenger_count\"] >0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c07dd2-d78b-4600-8923-1fc9f25d2713",
   "metadata": {
    "gather": {
     "logged": 1682234192111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower limit of dist is -1.8000000000000007 and upper limit of dist is 24.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1850598, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove outliers from cost / amount related columns\n",
    "\n",
    "df = df[(df['total_amount'] > 0) & (df['total_amount'] < 500)]\n",
    "df = df[df['extra'] >= 0]\n",
    "computer_remove_outliers(df, 'total_amount', 0)\n",
    "df = df[df['total_amount'] < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d19932-d22f-4ae3-b0a4-b7c184b58f70",
   "metadata": {
    "gather": {
     "logged": 1682234206890
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_22900/918374398.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_location['cluster_label'] = kmeans.fit_predict(df_location)  # df_location has lat and long in a DF.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster_label\n",
       "1    419711\n",
       "9    359259\n",
       "2    284075\n",
       "3    257172\n",
       "6    201107\n",
       "4    184058\n",
       "7     87519\n",
       "8     36611\n",
       "0     18638\n",
       "5      2448\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create clusters based on location\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_location = df[['pickup_latitude', 'pickup_longitude']]\n",
    "kmeans = KMeans(n_clusters = 10) \n",
    "df_location['cluster_label'] = kmeans.fit_predict(df_location)  # df_location has lat and long in a DF.\n",
    "df_location['cluster_label'].value_counts().sort_values(ascending=False)  # check count per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a83572-cb1f-4cfc-b45b-bdbb3bf254b1",
   "metadata": {
    "gather": {
     "logged": 1682234209270
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pickup_dayofweek</th>\n",
       "      <td>1679.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.001192</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_hour</th>\n",
       "      <td>1679.0</td>\n",
       "      <td>11.504467</td>\n",
       "      <td>6.923889</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.000</td>\n",
       "      <td>12.00</td>\n",
       "      <td>17.50</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_label</th>\n",
       "      <td>1679.0</td>\n",
       "      <td>4.499702</td>\n",
       "      <td>2.873967</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demand</th>\n",
       "      <td>1679.0</td>\n",
       "      <td>21.169839</td>\n",
       "      <td>20.995945</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.585</td>\n",
       "      <td>13.03</td>\n",
       "      <td>35.52</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count       mean        std   min    25%    50%    75%   \n",
       "pickup_dayofweek  1679.0   3.000000   2.001192  0.00  1.000   3.00   5.00  \\\n",
       "pickup_hour       1679.0  11.504467   6.923889  0.00  6.000  12.00  17.50   \n",
       "cluster_label     1679.0   4.499702   2.873967  0.00  2.000   4.00   7.00   \n",
       "demand            1679.0  21.169839  20.995945  0.02  2.585  13.03  35.52   \n",
       "\n",
       "                    max  \n",
       "pickup_dayofweek    6.0  \n",
       "pickup_hour        23.0  \n",
       "cluster_label       9.0  \n",
       "demand            100.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge location cluster with main dataframe\n",
    "\n",
    "df_with_loc_clusters = pd.merge(df_location, df, on=['pickup_latitude', 'pickup_longitude'])\n",
    "df_with_loc_clusters['pickup_hour'] = df_with_loc_clusters[\"tpep_pickup_datetime\"].dt.hour\n",
    "df_with_loc_clusters['pickup_dayofweek'] = df_with_loc_clusters[\"tpep_pickup_datetime\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b65d09-37d4-4418-8efd-2c559a179d6f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create demand column which is target column for the demand prediction model\n",
    "\n",
    "df_model = df_with_loc_clusters.groupby(['pickup_dayofweek','pickup_hour', 'cluster_label']).size().reset_index()\n",
    "df_model = df_model.rename(columns={0:'demand'})\n",
    "df_model['demand'] = df_model['demand']/df_model['demand'].max()\n",
    "df_model['demand'] = round((df_model['demand'] * 100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9d2ad2-d442-4dec-b831-7378db3183f3",
   "metadata": {
    "gather": {
     "logged": 1682234209818
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Model preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X = df_model.drop([\"demand\"],axis=1)\n",
    "y = df_model[['demand']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=100)\n",
    "\n",
    "cat_columns = ['pickup_dayofweek', 'pickup_hour', 'cluster_label']\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(X_train[cat_columns])\n",
    "\n",
    "X_train_encoded = enc.transform(X_train[cat_columns]).toarray()\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=enc.get_feature_names_out(cat_columns))\n",
    "X_train_model = pd.concat([X_train.reset_index(), X_train_encoded_df], axis=1).drop(['index'], axis=1)\n",
    "\n",
    "X_test_encoded = enc.transform(X_test[cat_columns]).toarray()\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=enc.get_feature_names_out(cat_columns))\n",
    "X_test_model = pd.concat([X_test.reset_index(), X_test_encoded_df], axis=1).drop(['index'], axis=1)\n",
    "\n",
    "X_train_model = X_train_model.drop(cat_columns, axis=1)\n",
    "X_test_model = X_test_model.drop(cat_columns, axis=1)\n",
    "\n",
    "X_train_model[X_train_model.columns] = X_train_model[X_train_model.columns].astype(int)\n",
    "X_test_model[X_test_model.columns] = X_test_model[X_test_model.columns].astype(int)\n",
    "\n",
    "y_train_model = y_train.reset_index().drop(['index'], axis=1)\n",
    "y_test_model = y_test.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "y_train_model = y_train_model['demand']\n",
    "y_test_model = y_test_model['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ea2a161-4a85-4b8c-b43b-dadbf7f7e6b8",
   "metadata": {
    "gather": {
     "logged": 1682234216454
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomForestRegressor(max_depth=8, max_features='sqrt', n_estimators=200)\n"
     ]
    }
   ],
   "source": [
    "# Hpyerparameter tuning for Random Forest algorithm\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "param_dist = { \n",
    "    'n_estimators': [200, 500, 1000],  # trees in forest\n",
    "    'min_samples_split': [2, 5, 7],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8]  \n",
    "}\n",
    "grid_search = RandomizedSearchCV(rf_model, param_distributions=param_dist, cv = 3, \n",
    "                                   verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_model, y_train_model)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "659b9a2b-cf9e-489a-985b-e6f5bee53ed7",
   "metadata": {
    "gather": {
     "logged": 1682234218988
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest on Train: MSE is 74.34 and R2 is 0.83\n",
      "\n",
      "\n",
      "random forest on Test: MSE is 87.39 and R2 is 0.8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training random forest using hyper parameters selected using randomized search CV.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "params = {\"max_depth\": 8, \"min_samples_split\": 7, \"n_estimators\": 1000}\n",
    "model_rf = RandomForestRegressor(**params)\n",
    "model_rf.fit(X_train_model , y_train_model) \n",
    "\n",
    "prediction_train = model_rf.predict(X_train_model)\n",
    "mse = mean_squared_error(prediction_train, y_train_model)\n",
    "r2 = model_rf.score(X_train_model,y_train_model)\n",
    "print('{}: MSE is {} and R2 is {}'.format('random forest on Train', round(mse,2), round(r2,2)))\n",
    "\n",
    "prediction = model_rf.predict(X_test_model)\n",
    "mse = mean_squared_error(prediction, y_test_model)\n",
    "r2 = model_rf.score(X_test_model,y_test_model)\n",
    "print('{}: MSE is {} and R2 is {}'.format('random forest on Test', round(mse,2), round(r2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a16b5d73-a83a-4932-8be5-cbb2dccdb3c2",
   "metadata": {
    "gather": {
     "logged": 1682234219808
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model_artifacts/taxi_demand_prediction.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and its artifacts\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(kmeans, \"../model_artifacts/kmeans.joblib\")\n",
    "joblib.dump(enc, \"../model_artifacts/enc.bin\", compress=True)\n",
    "joblib.dump(model_rf, \"../model_artifacts/taxi_demand_prediction.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e62746b-88f7-4389-bab0-d0546d72e3a5",
   "metadata": {
    "gather": {
     "logged": 1682234224741
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model taxi_demand_prediction\n"
     ]
    }
   ],
   "source": [
    "# Register model\n",
    "\n",
    "from azureml.core.model import Model\n",
    "import urllib.request\n",
    "\n",
    "model = Model.register(workspace, model_name=\"taxi_demand_prediction\", model_path=\"../model_artifacts/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb4cdc31-e2f5-4f59-81ab-f6e9a34dc489",
   "metadata": {
    "gather": {
     "logged": 1682234225492
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment(name=\"taxi_demand_prediction\")\n",
    "\n",
    "python_packages = ['azure-ml-api-sdk','numpy', 'pandas', 'seaborn', 'matplotlib', 'scipy', 'scikit-learn', 'joblib','requests']\n",
    "for package in python_packages:\n",
    "    env.python.conda_dependencies.add_pip_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5e94e-2856-4d29-9c2e-55b0c047daf8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Inference configuration setup\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    environment=env,\n",
    "    source_directory=\"../source_dir\",\n",
    "    entry_script=\"echo_score.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "345dcc63-be01-4da1-8d26-d3ab36948631",
   "metadata": {
    "gather": {
     "logged": 1682235642854
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Local Deployment\n",
    "\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace,\n",
    "    \"taxidemandprediction\",\n",
    "    [model],\n",
    "    inference_config,\n",
    "    deployment_config,\n",
    "    overwrite=True,\n",
    ")\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53db4107-3a2b-43f6-8c46-acb7e1522019",
   "metadata": {
    "gather": {
     "logged": 1682238916155
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demand percent is 61.33\n"
     ]
    }
   ],
   "source": [
    "# Local Testing\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "uri = service.scoring_uri\n",
    "requests.get(\"http://localhost:6789\")\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\"pickup_latitude\":\"-73.980492\",\"pickup_longitude\":\"40.777981\",\"tpep_pickup_datetime\":\"2020-02-13 23:40:00\"}\n",
    "data = json.dumps(data)\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4a8642-1bb2-47e5-a6f1-4db6b5c95d9a",
   "metadata": {
    "gather": {
     "logged": 1682237831786
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Local Testing Logs\n",
    "\n",
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b4221b2-6492-4ad3-95fe-f4c723745945",
   "metadata": {
    "gather": {
     "logged": 1682239023043
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Remote Deployment\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=0.5, memory_gb=1, auth_enabled=True\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace,\n",
    "    \"taxidemandprediction\",\n",
    "    [model],\n",
    "    inference_config,\n",
    "    deployment_config,\n",
    "    overwrite=True,\n",
    ")\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "836fa22b-36a4-4c91-9b21-4970fe3baad1",
   "metadata": {
    "gather": {
     "logged": 1682239337227
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"demand percent is 61.33\"\n"
     ]
    }
   ],
   "source": [
    "# Testing Remote Test Calls\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from azureml.core import Webservice\n",
    "\n",
    "service = Webservice(workspace=workspace, name=\"taxidemandprediction\")\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "key, _ = service.get_keys()\n",
    "\n",
    "# Set the appropriate headers\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "headers[\"Authorization\"] = f\"Bearer {key}\"\n",
    "\n",
    "# Make the request and display the response and logs\n",
    "data = {\"pickup_latitude\":\"-73.980492\",\"pickup_longitude\":\"40.777981\",\"tpep_pickup_datetime\":\"2020-02-13 23:40:00\"}\n",
    "\n",
    "data = json.dumps(data)\n",
    "resp = requests.post(scoring_uri, data=data, headers=headers)\n",
    "print(resp.text)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.030376,
   "end_time": "2022-12-17T06:47:03.220925",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-17T06:44:58.190549",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
