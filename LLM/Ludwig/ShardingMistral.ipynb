{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d05a2d-7356-4c45-b24d-04f4390ce0ae",
      "metadata": {
        "id": "56d05a2d-7356-4c45-b24d-04f4390ce0ae",
        "outputId": "c87d8297-1f7e-4955-ded2-140880bd16b9"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Token: ········\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = getpass.getpass(\"Token:\")\n",
        "assert os.environ[\"HUGGING_FACE_HUB_TOKEN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c55213-2864-4bb7-8f9c-4efd7c7d9553",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "526e7ce5fb8c44728e89624c267356ed",
            "4c1562c6f88d4f949da76eb07e6b1411",
            "ed9a2a27bf9441f1a669569fcdfd498d",
            "b75d926d7d954607824dd4b2556b04ad",
            "981aae0e4fd8409992282be4da669ba7",
            "874e04b933614ac08906cf17a58539a1",
            "a6c39241b47f43b08ab9804447413ffc",
            "437cf8b3597040b69f89d315fa671379",
            "bf08b75fd1b04a2e934a31923dbf0cb2",
            "4b9049aee00a4888803ee2f24d58304c"
          ]
        },
        "id": "76c55213-2864-4bb7-8f9c-4efd7c7d9553",
        "outputId": "7c762b71-c172-4010-a1b9-de1ceaee5a8d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "526e7ce5fb8c44728e89624c267356ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c1562c6f88d4f949da76eb07e6b1411",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00008.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed9a2a27bf9441f1a669569fcdfd498d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 8 LFS files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b75d926d7d954607824dd4b2556b04ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00005-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "981aae0e4fd8409992282be4da669ba7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00004-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "874e04b933614ac08906cf17a58539a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6c39241b47f43b08ab9804447413ffc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "437cf8b3597040b69f89d315fa671379",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00006-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf08b75fd1b04a2e934a31923dbf0cb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00007-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b9049aee00a4888803ee2f24d58304c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00008-of-00008.bin:   0%|          | 0.00/816M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('C:\\\\Users\\\\svij\\\\Downloads\\\\Mistral-7B-v0.1-sharded\\\\tokenizer_config.json',\n",
              " 'C:\\\\Users\\\\svij\\\\Downloads\\\\Mistral-7B-v0.1-sharded\\\\special_tokens_map.json',\n",
              " 'C:\\\\Users\\\\svij\\\\Downloads\\\\Mistral-7B-v0.1-sharded\\\\tokenizer.json')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## load the Mistral model into memory\n",
        "original_base_model_tokenizer = AutoTokenizer.from_pretrained(\n",
        "  pretrained_model_name_or_path=\"mistralai/Mistral-7B-v0.1\",\n",
        "  trust_remote_code=True,\n",
        "  padding_side=\"left\"\n",
        ")\n",
        "original_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "  pretrained_model_name_or_path=\"mistralai/Mistral-7B-v0.1\",\n",
        "  device_map=\"auto\",  # enables using CPU or disk to load the model based on availability.\n",
        "  torch_dtype=torch.float16,\n",
        "  trust_remote_code=True,\n",
        "  low_cpu_mem_usage=True  # enables loading the model into CPU\n",
        "  # offload_folder='offload'  # In case we provide an offload folder, we are allowing the model to spill over to disk in case CPU isn't sufficient to hold on to the model. Default location will be - C:\\Users\\<uname>\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.1\\snapshots\\<somealphanumericcode>\n",
        "  )\n",
        "\n",
        "## Create model shards. The shards are loaded locally and also to hugging face repo.\n",
        "original_base_model.save_pretrained(\n",
        "  save_directory=\"C:\\\\Users\\\\<uname>\\\\Downloads\\\\Mistral-7B-v0.1-sharded\",\n",
        "  max_shard_size=\"2GB\",  # shards of max size 2GB will be created.\n",
        "  push_to_hub=True,  # model shards will be moved to hugging face hub\n",
        "  repo_id=\"Siddharthvij10/MistralSharded2\"  # hugging face repo id\n",
        ")\n",
        "original_base_model_tokenizer.save_pretrained(\n",
        "  save_directory=\"C:\\\\Users\\\\<uname>\\\\Downloads\\\\Mistral-7B-v0.1-sharded\",\n",
        "  legacy_format=False,\n",
        "  push_to_hub=True,\n",
        "  repo_id=\"Siddharthvij10/MistralSharded2\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}