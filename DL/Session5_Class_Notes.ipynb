{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKb3KfJgzpun"
      },
      "source": [
        "# !pip install torch --quiet\n",
        "# !pip uninstall torchtext -y\n",
        "# !pip install torchtext==0.10.0 --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pXFnhhMzx4R"
      },
      "source": [
        "from torchtext.datasets import AG_NEWS"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3sWLqLmz2tp",
        "outputId": "e3c651b6-8959-4b59-919f-cad4c06f555a"
      },
      "source": [
        "# !pip install torchdata\n",
        "train_iter = AG_NEWS(split='train')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train.csv: 29.5MB [00:00, 67.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev1nEGyC0JVd",
        "outputId": "5a2fe2f3-b69d-4203-f86e-a4a0cf1fa3ef"
      },
      "source": [
        "next(train_iter)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrB4PfQA0MQi",
        "outputId": "52fcabcc-7b3d-40ca-cff2-2b5fe6822daa"
      },
      "source": [
        "for (line_number, (label, line)) in enumerate(train_iter):\n",
        "  print(label, line)\n",
        "  if line_number == 19:\n",
        "    break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "3 Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "3 Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "3 Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "3 Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n",
            "3 Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\n",
            "3 Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.\n",
            "3 Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.\n",
            "3 Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\n",
            "3 Oil and Economy Cloud Stocks' Outlook  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market next week during the depth of the  summer doldrums.\n",
            "3 No Need for OPEC to Pump More-Iran Gov  TEHRAN (Reuters) - OPEC can do nothing to douse scorching  oil prices when markets are already oversupplied by 2.8 million  barrels per day (bpd) of crude, Iran's OPEC governor said  Saturday, warning that prices could fall sharply.\n",
            "3 Non-OPEC Nations Should Up Output-Purnomo  JAKARTA (Reuters) - Non-OPEC oil exporters should consider  increasing output to cool record crude prices, OPEC President  Purnomo Yusgiantoro said on Sunday.\n",
            "3 Google IPO Auction Off to Rocky Start  WASHINGTON/NEW YORK (Reuters) - The auction for Google  Inc.'s highly anticipated initial public offering got off to a  rocky start on Friday after the Web search company sidestepped  a bullet from U.S. securities regulators.\n",
            "3 Dollar Falls Broadly on Record Trade Gap  NEW YORK (Reuters) - The dollar tumbled broadly on Friday  after data showing a record U.S. trade deficit in June cast  fresh doubts on the economy's recovery and its ability to draw  foreign capital to fund the growing gap.\n",
            "3 Rescuing an Old Saver If you think you may need to help your elderly relatives with their finances, don't be shy about having the money talk -- soon.\n",
            "3 Kids Rule for Back-to-School The purchasing power of kids is a big part of why the back-to-school season has become such a huge marketing phenomenon.\n",
            "3 In a Down Market, Head Toward Value Funds There is little cause for celebration in the stock market these days, but investors in value-focused mutual funds have reason to feel a bit smug -- if only because they've lost less than the folks who stuck with growth.\n",
            "3 US trade deficit swells in June The US trade deficit has exploded 19 to a record \\$55.8bn as oil costs drove imports higher, according to a latest figures.\n",
            "3 Shell 'could be target for Total' Oil giant Shell could be bracing itself for a takeover attempt, possibly from French rival Total, a  press report claims.\n",
            "3 Google IPO faces Playboy slip-up The bidding gets underway for Google's public offering, despite last-minute worries over an interview with its bosses in Playboy magazine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7LwnrU-0cMI"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_iter = AG_NEWS(split = 'train')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_vsHPV_1TZ-"
      },
      "source": [
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_4z8hD41_fI",
        "outputId": "6c5741f7-a386-4d21-b91b-e1ab03663458"
      },
      "source": [
        "next(iter(dataloader))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([3, 3, 3, 3, 3, 3, 3, 3]),\n",
              " (\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
              "  'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
              "  \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\",\n",
              "  'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
              "  'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.',\n",
              "  'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)',\n",
              "  \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\",\n",
              "  'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq-IuEKY2GQR"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdxunJbc3FUH",
        "outputId": "25da0e0d-1316-420a-8ee7-707146b81b54"
      },
      "source": [
        "help(get_tokenizer)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function get_tokenizer in module torchtext.data.utils:\n",
            "\n",
            "get_tokenizer(tokenizer, language='en')\n",
            "    Generate tokenizer function for a string sentence.\n",
            "    \n",
            "    Args:\n",
            "        tokenizer: the name of tokenizer function. If None, it returns split()\n",
            "            function, which splits the string sentence by space.\n",
            "            If basic_english, it returns _basic_english_normalize() function,\n",
            "            which normalize the string first and split by space. If a callable\n",
            "            function, it will return the function. If a tokenizer library\n",
            "            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
            "            corresponding library.\n",
            "        language: Default en\n",
            "    \n",
            "    Examples:\n",
            "        >>> import torchtext\n",
            "        >>> from torchtext.data import get_tokenizer\n",
            "        >>> tokenizer = get_tokenizer(\"basic_english\")\n",
            "        >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
            "        >>> tokens\n",
            "        >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y2YR4UV3GOi"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNW8jB3h3cay",
        "outputId": "8db5a99c-889d-497e-a2b3-ff286c928560"
      },
      "source": [
        "tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
        "tokens"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKg_ePFP3emp",
        "outputId": "17da45e7-6a9b-4ac6-8aec-125faeae8b2e"
      },
      "source": [
        "help(build_vocab_from_iterator)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function build_vocab_from_iterator in module torchtext.vocab:\n",
            "\n",
            "build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True) -> torchtext.vocab.Vocab\n",
            "    Build a Vocab from an iterator.\n",
            "    \n",
            "    Args:\n",
            "        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
            "        min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
            "        specials: Special symbols to add. The order of supplied tokens will be preserved.\n",
            "        special_first: Indicates whether to insert symbols at the beginning or at the end.\n",
            "    \n",
            "    \n",
            "    Returns:\n",
            "        torchtext.vocab.Vocab: A `Vocab` object\n",
            "    \n",
            "    Examples:\n",
            "        >>> #generating vocab from text file\n",
            "        >>> import io\n",
            "        >>> from torchtext.vocab import build_vocab_from_iterator\n",
            "        >>> def yield_tokens(file_path):\n",
            "        >>>     with io.open(file_path, encoding = 'utf-8') as f:\n",
            "        >>>         for line in f:\n",
            "        >>>             yield line.strip().split()\n",
            "        >>> vocab = build_vocab_from_iterator(yield_tokens_batch(file_path), specials=[\"<unk>\"])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALqqil43mX2"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = AG_NEWS(split = 'train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r4fhQZ85Piw"
      },
      "source": [
        "def my_func():\n",
        "  print(\"Hello! Welcome to the Dothraki Kingdon\")\n",
        "  yield\n",
        "  print(\"Have you brought some Gold for us?\")\n",
        "  yield\n",
        "  print(\"NO? Then you must die!!!\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4b6NXOg5VZY",
        "outputId": "34301af4-51f2-4b6c-cf88-8adbe8e7c54b"
      },
      "source": [
        "temp_my_func = my_func()\n",
        "next(temp_my_func)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Welcome to the Dothraki Kingdon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHYpIkNx5khJ",
        "outputId": "93adc804-2013-470e-d3ca-12b78f239f42"
      },
      "source": [
        "next(temp_my_func)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Have you brought some Gold for us?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM6Rneo55rGS"
      },
      "source": [
        "# next(temp_my_func)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Cp7Qbw6xS1"
      },
      "source": [
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OKrep6I7JqF",
        "outputId": "934d5cb3-586e-405f-d8d8-b4dc5f0ae8aa"
      },
      "source": [
        "vocab(['here', 'is', 'an', 'example', 'of', 'alien', 'invasion', 'they', 'are', 'called', 'bangalorites'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[475, 21, 30, 5297, 6, 7838, 3042, 67, 42, 440, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQVgPe4P7Xef",
        "outputId": "d71cb950-b91a-4299-97a5-22b3482e2408"
      },
      "source": [
        "vocab[\"<unk>\"]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK1sxfsM7doh"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iiJdwCO79Yx",
        "outputId": "ec95a56a-b014-46dd-e612-6671c0e30f8d"
      },
      "source": [
        "# text_pipeline(\"Here is an exmaple of alient invasion, and they are called bangaloriters\")\n",
        "text_pipeline(\"No\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[78]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TQusm2u8COn",
        "outputId": "5eef7e21-5155-4ceb-e318-4ad79f3ec489"
      },
      "source": [
        "label_pipeline('19')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xArz2o1P8MKG"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def collate_fn(batch):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  for src_batch, tgt_batch in batch:\n",
        "    src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip('\\n')))\n",
        "    tgt_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip('\\n')))\n",
        "  src_batch = pad_sequences(src_batch, padding_value=PAD_IDX)\n",
        "  tgt_batch = pad_sequences(tgt_batch, padding_value=PAD_IDX)\n",
        "  return src_batch, tgt_batch"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFbwLqoN9K9p",
        "outputId": "9c708421-c5e1-4677-f6b2-a3d2c74ce036"
      },
      "source": [
        "weight = torch.randn(10, 5)\n",
        "weight"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6809e+00, -4.8683e-01, -1.1393e+00,  3.9214e-02,  2.0237e-01],\n",
              "        [ 4.9862e-01, -1.0139e+00,  1.5631e+00, -1.4300e+00,  6.5936e-01],\n",
              "        [-7.3319e-01, -5.6577e-01,  6.0029e-01, -3.6710e-01,  6.1115e-02],\n",
              "        [ 7.1037e-01,  2.8070e-01,  1.2768e+00,  6.1373e-01,  1.2786e+00],\n",
              "        [-4.9051e-01,  6.0958e-02, -2.0721e-01, -1.0211e+00, -1.6249e+00],\n",
              "        [-5.5844e-01, -1.0487e-01, -5.2271e-01,  2.1942e-01,  1.3382e+00],\n",
              "        [ 5.3808e-01, -1.1331e+00, -7.8348e-02, -1.0838e+00, -4.2695e-02],\n",
              "        [ 1.7687e-03, -1.2634e+00, -1.7945e+00,  1.8705e+00,  2.5631e+00],\n",
              "        [-1.4282e+00, -2.8475e-01, -4.6216e-01,  1.3882e+00,  3.9477e-02],\n",
              "        [ 6.6591e-01, -1.1809e+00,  1.2942e+00, -2.4367e-01,  1.3181e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCobVaQ49umS",
        "outputId": "8f97b117-454b-45a8-f493-ebfef8ff5120"
      },
      "source": [
        "indices = torch.tensor([4, 1, 7])\n",
        "indices"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 1, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0texNYh90c6",
        "outputId": "15041bcd-ffc7-4620-b8d8-e0057a44ca9b"
      },
      "source": [
        "embeddings = torch.nn.functional.embedding(indices, weight)\n",
        "embeddings"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.9051e-01,  6.0958e-02, -2.0721e-01, -1.0211e+00, -1.6249e+00],\n",
              "        [ 4.9862e-01, -1.0139e+00,  1.5631e+00, -1.4300e+00,  6.5936e-01],\n",
              "        [ 1.7687e-03, -1.2634e+00, -1.7945e+00,  1.8705e+00,  2.5631e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1z-FVN19_VW",
        "outputId": "9405e81e-aa13-45eb-8ba7-a007a0b979a9"
      },
      "source": [
        "embeddings.mean(dim=0, keepdim=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0033, -0.7388, -0.1462, -0.1935,  0.5325]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrS2kb8p-i1Q",
        "outputId": "2feeb2db-323c-43d2-e246-16516b56d04e"
      },
      "source": [
        "torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode=\"mean\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0033, -0.7388, -0.1462, -0.1935,  0.5325]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSeLl_Sh-6ON"
      },
      "source": [
        "import sys\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    # print(((batch[0][1])))  # len(batch) = 64 i.e 1 batch is a list containing 64 elements. batch[0] is of length 2 with the first one containing label and second containing sentence\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        #  print(processed_text)\n",
        "        #  print(label_list)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "        #  print(offsets)\n",
        "        #  print(text_list)\n",
        "        #  sys.exit()\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)  \n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    # print(len(text_list))\n",
        "    text_list = torch.cat(text_list)  # make a long long list containing numeric rep of all sentences one after another.\n",
        "    # print(len(text_list))\n",
        "    # sys.exit()\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRDpOkXiAqyo"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-IOBmF7A1N-"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        # torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode=\"mean\")\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)  # making a custom embedding with embedding of dim as \"embed_dim\"\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKuQGv-xBIC1"
      },
      "source": [
        "1: World, 2: Sports, 3 Business, 4 Sci/Tech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW1rS0OYBGhR"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVCypgvBQfM"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "        optimizer.step()\n",
        "        # print(\"predicted label and label is \", predited_label, label)\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        # print(\"total_count and total_acc is \", total_count, total_acc )\n",
        "        # sys.exit()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7lPedpBfpS",
        "outputId": "ba3534e7-e02f-452b-988f-c09c4748142b"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test.csv: 1.86MB [00:00, 36.7MB/s]                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.690\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.856\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.876\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 18.23s | valid accuracy    0.887 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.898\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.901\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.904\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 17.69s | valid accuracy    0.893 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.914\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.914\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.913\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 17.29s | valid accuracy    0.905 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.927\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.921\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.923\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 20.98s | valid accuracy    0.910 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.929\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.930\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.930\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 17.49s | valid accuracy    0.893 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.943\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.945\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.940\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 17.69s | valid accuracy    0.911 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.944\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.943\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 19.24s | valid accuracy    0.912 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.944\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.945\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.944\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 19.50s | valid accuracy    0.911 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.950\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.946\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.943\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 17.54s | valid accuracy    0.912 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.946\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.947\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.946\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 18.02s | valid accuracy    0.912 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUrnPMhVBlq9",
        "outputId": "de3fbd52-e67c-4915-8e7d-f1c95deb28be"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76PGhuKbB4BN",
        "outputId": "97b66beb-ada1-4e07-9adb-f96a192f1516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Sports news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-bXp1P9B55k"
      },
      "source": [],
      "execution_count": 38,
      "outputs": []
    }
  ]
}