{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsRgzxDqBD/419ua14aQ6N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE-neeOVDBpo",
        "outputId": "6ae8950c-3dc2-4568-97bc-6a3a1485cca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.7981,  0.6570,  0.7857,  0.0070, -1.6912],\n",
            "         [-1.6480, -0.3626, -0.2469,  0.8470, -1.3059],\n",
            "         [-0.0124, -0.5408,  0.6233,  1.0897,  1.3376],\n",
            "         [-0.7591,  0.5101,  0.7842, -1.1904, -0.9384]]]) torch.Size([1, 4, 5])\n",
            "tensor([[[0., 0., 0.]]]) torch.Size([1, 1, 3])\n",
            "tensor([[[ 0.6484, -0.4496,  0.7094],\n",
            "         [ 0.6600,  0.6924,  0.9412],\n",
            "         [-0.6979,  0.5795, -0.3076],\n",
            "         [ 0.3527, -0.4096,  0.4238]]], grad_fn=<TransposeBackward1>) torch.Size([1, 4, 3])\n",
            "tensor([[[ 0.3527, -0.4096,  0.4238]]], grad_fn=<StackBackward0>) torch.Size([1, 1, 3])\n",
            "tensor([[-0.1046]], grad_fn=<AddmmBackward0>) torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Create RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        # Number of hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Number of hidden layers\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # RNN\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='tanh')  # bias is by default true in RNN hidden layer; \n",
        "        #   tanh and relu are 2 options for activations that can be used in RNN. default is tanh; batch_first means that u are passing data in format \n",
        "        #   (batch_size, timestep/seq , features)\n",
        "        \n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):  # ????????where are the initial weights being initialized.\n",
        "        \n",
        "        # print(x.shape, x, '\\n')  # prints the initial values of data getting passed - shape - batch, words, feature\n",
        "\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim)  # batch_size = x.size(0)\n",
        "\n",
        "        # One time step\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        # print(x, x.shape)\n",
        "        # print(h0, h0.shape)\n",
        "        # print(out, out.shape)\n",
        "        # print(hn, hn.shape)\n",
        "\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        print(out, out.shape)\n",
        "        # out = self.fc(out[:, -1, :]) \n",
        "        return out\n",
        "\n",
        "# Create RNN\n",
        "input_dim = 5    # input dimension - same as vocab length\n",
        "hidden_dim = 3  # hidden layer dimension\n",
        "n_layers = 1     # number of hidden layers\n",
        "output_dim = 1   # output dimension - sentiment analysis has either positive or negative sentiment so, one output\n",
        "batch_size = 1\n",
        "sequence_length = 4\n",
        "\n",
        "model = RNNModel(input_dim, hidden_dim, n_layers, output_dim)\n",
        "print(model, '\\n')\n",
        "\n",
        "# model[0].state_dict() # shows parameters  # model[0].weight.data , model[0].bias.data are other ways to get parameters\n",
        "## shows hidden weight and hidden bias for RNN; shows w and b for fc\n",
        "# for name, param in model.named_parameters():\n",
        "#   print(name)  \n",
        "## shows hidden weight and hidden bias for RNN; shows w and b for fc. Shows sample weights as well\n",
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param.data)  # 5*3 not 3*5; same is issue with hidden weights\n",
        "\n",
        "# from torchsummary import summary\n",
        "ip = torch.randn(batch_size, sequence_length, input_dim)\n",
        "output = model(ip)"
      ]
    }
  ]
}