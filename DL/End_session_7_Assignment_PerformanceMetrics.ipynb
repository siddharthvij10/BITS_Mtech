{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVDLsoFs0j/mREUbPvkHr/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Tcz7bXHGz9Lb",
        "outputId": "f0a6de80-cac6-41d7-d8e2-fba7f52cc3bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 12:07:26--  http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8254496 (7.9M) [application/x-gzip]\n",
            "Saving to: ‘Question_Answer_Dataset_v1.2.tar.gz.3’\n",
            "\n",
            "Question_Answer_Dat 100%[===================>]   7.87M   762KB/s    in 15s     \n",
            "\n",
            "2023-05-05 12:07:41 (535 KB/s) - ‘Question_Answer_Dataset_v1.2.tar.gz.3’ saved [8254496/8254496]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./data/question_answer_pairs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "# !pip install torch\n",
        "import torch\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "!wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\n",
        "  \n",
        "file = tarfile.open('Question_Answer_Dataset_v1.2.tar.gz')  # open file\n",
        "file.extractall('./data') # extract files  \n",
        "file.close()# close file\n",
        "\n",
        "file_path = './data/question_answer_pairs.txt'\n",
        "shutil.move(\"./data/Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\", file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readData(file_path):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('{}'.format(file_path), encoding='latin-1').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[1:3]] for l in lines]\n",
        "    pairs = pairs[1:]\n",
        "\n",
        "    question_details = Data('question')  # CHECK\n",
        "    answer_details = Data('answer')  # CHECK\n",
        "\n",
        "    return question_details, answer_details, pairs\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def prepareData(file_path):\n",
        "    question_details, answer_details, pairs = readData(file_path)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        question_details.addSentence(pair[0])\n",
        "        answer_details.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(question_details.name, question_details.n_words)\n",
        "    print(answer_details.name, answer_details.n_words)\n",
        "    return question_details, answer_details, pairs\n",
        "\n",
        "\n",
        "question_details, answer_details, pairs = prepareData(file_path)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-fNavSQbJt2",
        "outputId": "ab633de1-b6ca-4164-d723-58224fe9b02e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 1715 sentence pairs\n",
            "Trimmed to 926 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "question 992\n",
            "answer 670\n",
            "['what is the largest living species of penguin ?', 'emperor penguin']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (pairs)  # remove ? and . from end. After that, remove whitestapces from end\n",
        "\n",
        "answer_details.name, answer_details.n_words, question_details.name, question_details.n_words\n",
        "# (answer_details.word2index, answer_details.index2word, answer_details.word2count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2bT1qOwUefH",
        "outputId": "735fbf68-baf0-4c99-8b38-c9157b8a674b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('answer', 670, 'question', 992)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(question_details, pair[0])\n",
        "    target_tensor = tensorFromSentence(answer_details, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "TN3q-9bVdJQL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # this is SOS token\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "YOpz62VWctJh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score==0.3.13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRnXXxZgYm03",
        "outputId": "93fd36f3-7951-446f-b559-2a61ac62665c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert_score==0.3.13 in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (1.22.4)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (2.27.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (2.0.0+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (1.5.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score==0.3.13) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score==0.3.13) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score==0.3.13) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score==0.3.13) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score==0.3.13) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score==0.3.13) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score==0.3.13) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score==0.3.13) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score==0.3.13) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score==0.3.13) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score==0.3.13) (3.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score==0.3.13) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score==0.3.13) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score==0.3.13) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score==0.3.13) (0.14.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score==0.3.13) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score==0.3.13) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score==0.3.13) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score==0.3.13) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score==0.3.13) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score==0.3.13) (0.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score==0.3.13) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score==0.3.13) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score==0.3.13) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score==0.3.13) (2.0.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=3.0.0->bert_score==0.3.13) (2023.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score==0.3.13) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score==0.3.13) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score==0.3.13) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "teacher_forcing_ratio = 0.25\n",
        "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "\n",
        "def unigram_precision(predicted_tensor, target_tensor):\n",
        "    predicted_tensor_flatten = torch.flatten(predicted_tensor)\n",
        "    predicted_tensor_frequencies = torch.bincount(predicted_tensor_flatten.type(torch.int64))\n",
        "    total_predicted_tensor_frequencies = torch.sum(predicted_tensor_frequencies)  # Denominator for unigram precision computation.\n",
        "\n",
        "    target_tensor_flatten = torch.flatten(target_tensor)\n",
        "    target_tensor_frequencies = torch.bincount(target_tensor_flatten.type(torch.int64))\n",
        "    \n",
        "    len_pred = list(predicted_tensor_frequencies.shape)[0]\n",
        "    len_target = list(target_tensor_frequencies.shape)[0]\n",
        "\n",
        "    # make_len_equal_target_pred_by_appending_zeros\n",
        "    if len_pred > len_target:  # len of pred is more than len of actual\n",
        "        target_tensor_frequencies = nn.ConstantPad1d((0, len_pred - len_target), 0)(target_tensor_frequencies)\n",
        "    elif len_pred < len_target:\n",
        "        predicted_tensor_frequencies = nn.ConstantPad1d((0, len_target - len_pred), 0)(predicted_tensor_frequencies)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    min_pred_target_frequencies = torch.min(predicted_tensor_frequencies, target_tensor_frequencies)  # Clip count - keep min of pred and actual freq\n",
        "    total_min_pred_target_tensor_frequencies = torch.sum(min_pred_target_frequencies)  # Numerator for unigram precision computation.\n",
        "\n",
        "    try:\n",
        "        unigram_precision_val = total_min_pred_target_tensor_frequencies.item() / total_predicted_tensor_frequencies.item()\n",
        "    except ZeroDivisionError:\n",
        "        unigram_precision_val = 0\n",
        "\n",
        "    return unigram_precision_val\n",
        "\n",
        "def brevity_panelty(predicted_target_trimmed, target_tensor_trimmed):  # penalize machine output which is of lesser length than actual output\n",
        "    count_elements_in_predicted_target_trimmed = predicted_target_trimmed.numel()\n",
        "    count_elements_in_target_tensor_trimmed = target_tensor_trimmed.numel()\n",
        "    if int(count_elements_in_predicted_target_trimmed) < int(count_elements_in_target_tensor_trimmed):\n",
        "        try:\n",
        "            penalty = math.exp((1-(count_elements_in_target_tensor_trimmed/count_elements_in_predicted_target_trimmed)))\n",
        "        except ZeroDivisionError:\n",
        "            penalty = math.exp(1)\n",
        "        return penalty\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    prob_for_perplexity = []\n",
        "\n",
        "    if False:  # use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        predicted_target = torch.zeros(max_length, 1, device=device)  \n",
        "\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            \n",
        "            prob_for_perplexity.append(topv.squeeze())\n",
        "\n",
        "            decoder_input_for_precision = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            predicted_target[di] = decoder_input_for_precision  # evaluation\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "        perplexity = math.exp((-1/target_length) * sum(prob_for_perplexity))  # Compute Perplexity\n",
        "\n",
        "        predicted_target_trimmed = predicted_target[predicted_target.sum(dim=1) != 0]  # remove unwanted rows at the end with 0 value.\n",
        "        predicted_target_trimmed = predicted_target_trimmed[predicted_target_trimmed[:, 0] !=1]  # remove last row in case it has EOS token\n",
        "        row_exclude = target_tensor.shape[0]-1\n",
        "        target_tensor_trimmed = torch.cat((target_tensor[:row_exclude],target_tensor[row_exclude+1:]))  # remove EOS token from end\n",
        "\n",
        "        ### Compute Bert Score between Predicted and Target Sent\n",
        "\n",
        "        decoded_words = []  # Predicted Sentence\n",
        "        decoded_sentence_list = []\n",
        "\n",
        "        # Preperaing predicted tensor for BERT score.\n",
        "        if predicted_target_trimmed.shape == (1,1):\n",
        "            predicted_target_trimmed_Reshaped = predicted_target_trimmed.squeeze().unsqueeze(0)\n",
        "        else:\n",
        "            predicted_target_trimmed_Reshaped = predicted_target_trimmed.squeeze()\n",
        "\n",
        "        for predicted_target_index in (predicted_target_trimmed_Reshaped).type('torch.IntTensor'):\n",
        "            decoded_words.append(answer_details.index2word[predicted_target_index.item()])\n",
        "\n",
        "        decoded_sentence = ' '.join(decoded_words)\n",
        "        decoded_sentence_list.append(decoded_sentence)\n",
        "        target_words = []\n",
        "        target_sentence_list = []\n",
        "\n",
        "        # Preperaing target tensor for BERT score.\n",
        "        if target_tensor_trimmed.shape == (1,1):\n",
        "            target_tensor_trimmed_Reshaped = target_tensor_trimmed.squeeze().unsqueeze(0)\n",
        "        else:\n",
        "            target_tensor_trimmed_Reshaped = target_tensor_trimmed.squeeze()\n",
        "\n",
        "        for target_tensor in (target_tensor_trimmed_Reshaped).type('torch.IntTensor'):\n",
        "            target_words.append(answer_details.index2word[target_tensor.item()])\n",
        "\n",
        "        target_sentence = ' '.join(target_words)\n",
        "        target_sentence_list.append(target_sentence)\n",
        "\n",
        "        if (decoded_sentence_list[0] in (' ', '')) or (target_sentence_list[0] in (' ', '')):\n",
        "            BERT_P, BERT_R, BERT_F1 = 0, 0, 0\n",
        "        else:\n",
        "            BERT_P, BERT_R, BERT_F1 = scorer.score(decoded_sentence_list, target_sentence_list) \n",
        "\n",
        "        # Calculate unigram precision for predicted and target. Continue to compute penalty and BLEU score.\n",
        "        unigram_precision_value = unigram_precision(predicted_target_trimmed, target_tensor_trimmed)\n",
        "        modified_n_gram_precision = math.sqrt(unigram_precision_value)  # n is 2 - square root - since we are using unigram and bigrams.\n",
        "        brevity_panelty_val = brevity_panelty(predicted_target_trimmed, target_tensor_trimmed)\n",
        "        BLEU = brevity_panelty_val * modified_n_gram_precision\n",
        "\n",
        "        # intersection of pred and target to get correct prediction count\n",
        "        indices = torch.zeros_like(target_tensor_trimmed, dtype = torch.uint8, device = 'cuda')\n",
        "        for elem in predicted_target_trimmed:\n",
        "            indices = indices | (target_tensor_trimmed == elem)  \n",
        "        intersection = target_tensor_trimmed[indices]  \n",
        "        indices_backward = torch.zeros_like(predicted_target_trimmed, dtype = torch.uint8, device = 'cuda')\n",
        "        for elem in target_tensor_trimmed:\n",
        "            indices_backward = indices_backward | (predicted_target_trimmed == elem)  \n",
        "        intersection_backward = predicted_target_trimmed[indices_backward]\n",
        "\n",
        "        correct_predictions_count = min(intersection.shape[0], intersection_backward.shape[0])\n",
        "\n",
        "        # Generic Precision value and Recall value\n",
        "        total_predictions_count = predicted_target_trimmed.shape[0]\n",
        "        if total_predictions_count != 0:\n",
        "            precision = correct_predictions_count / total_predictions_count\n",
        "        else:\n",
        "            precision = 0\n",
        "        total_actual_count = target_tensor_trimmed.shape[0]\n",
        "        recall = correct_predictions_count / total_actual_count\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        predicted_target = torch.zeros(max_length, 1, device=device)  # evaluation\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            prob_for_perplexity.append(topv.squeeze())\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            predicted_target[di] = decoder_input  # evaluation\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "        perplexity = math.exp((-1/target_length) * sum(prob_for_perplexity))\n",
        "\n",
        "        predicted_target_trimmed = predicted_target[predicted_target.sum(dim=1) != 0]  # remove unwanted rows at the end with 0 value.\n",
        "        predicted_target_trimmed = predicted_target_trimmed[predicted_target_trimmed[:, 0] !=1]  # remove last row in case it has EOS token\n",
        "        row_exclude = target_tensor.shape[0]-1\n",
        "        target_tensor_trimmed = torch.cat((target_tensor[:row_exclude],target_tensor[row_exclude+1:]))  # remove EOS token from end\n",
        "\n",
        "        ### Compute Bert Score between Predicted and Target Sent\n",
        "\n",
        "        decoded_words = []  # Predicted Sentence\n",
        "        decoded_sentence_list = []\n",
        "\n",
        "        # Preperaing predicted tensor for BERT score.\n",
        "        if predicted_target_trimmed.shape == (1,1):\n",
        "            predicted_target_trimmed_Reshaped = predicted_target_trimmed.squeeze().unsqueeze(0)\n",
        "        else:\n",
        "            predicted_target_trimmed_Reshaped = predicted_target_trimmed.squeeze()\n",
        "\n",
        "        for predicted_target_index in (predicted_target_trimmed_Reshaped).type('torch.IntTensor'):\n",
        "            decoded_words.append(answer_details.index2word[predicted_target_index.item()])\n",
        "\n",
        "        decoded_sentence = ' '.join(decoded_words)\n",
        "        decoded_sentence_list.append(decoded_sentence)\n",
        "        target_words = []\n",
        "        target_sentence_list = []\n",
        "\n",
        "        # Preperaing target tensor for BERT score.\n",
        "        if target_tensor_trimmed.shape == (1,1):\n",
        "            target_tensor_trimmed_Reshaped = target_tensor_trimmed.squeeze().unsqueeze(0)\n",
        "        else:\n",
        "            target_tensor_trimmed_Reshaped = target_tensor_trimmed.squeeze()\n",
        "\n",
        "        for target_tensor in (target_tensor_trimmed_Reshaped).type('torch.IntTensor'):\n",
        "            target_words.append(answer_details.index2word[target_tensor.item()])\n",
        "\n",
        "        target_sentence = ' '.join(target_words)\n",
        "        target_sentence_list.append(target_sentence)\n",
        "\n",
        "        if (decoded_sentence_list[0] in (' ', '')) or (target_sentence_list[0] in (' ', '')):\n",
        "            BERT_P, BERT_R, BERT_F1 = 0, 0, 0\n",
        "        else:\n",
        "            BERT_P, BERT_R, BERT_F1 = scorer.score(decoded_sentence_list, target_sentence_list)  # , model_type=MODEL_TYPE)\n",
        "\n",
        "        # Calculate unigram precision for predicted and target. Continue to compute penalty and BLEU score.\n",
        "        unigram_precision_value = unigram_precision(predicted_target_trimmed, target_tensor_trimmed)\n",
        "        modified_n_gram_precision = math.sqrt(unigram_precision_value)  # n is 2 - square root - since we are using unigram and bigrams.\n",
        "        brevity_panelty_val = brevity_panelty(predicted_target_trimmed, target_tensor_trimmed)\n",
        "        BLEU = brevity_panelty_val * modified_n_gram_precision\n",
        "\n",
        "        # intersection of pred and target to get correct prediction count\n",
        "        indices = torch.zeros_like(target_tensor_trimmed, dtype = torch.uint8, device = 'cuda')\n",
        "        for elem in predicted_target_trimmed:\n",
        "            indices = indices | (target_tensor_trimmed == elem)  \n",
        "        intersection = target_tensor_trimmed[indices]  \n",
        "        indices_backward = torch.zeros_like(predicted_target_trimmed, dtype = torch.uint8, device = 'cuda')\n",
        "        for elem in target_tensor_trimmed:\n",
        "            indices_backward = indices_backward | (predicted_target_trimmed == elem)  \n",
        "        intersection_backward = predicted_target_trimmed[indices_backward]\n",
        "\n",
        "        correct_predictions_count = min(intersection.shape[0], intersection_backward.shape[0])\n",
        "\n",
        "        # Generic Precision value and Recall value\n",
        "        total_predictions_count = predicted_target_trimmed.shape[0]\n",
        "        if total_predictions_count != 0:\n",
        "            precision = correct_predictions_count / total_predictions_count\n",
        "        else:\n",
        "            precision = 0\n",
        "\n",
        "        total_actual_count = target_tensor_trimmed.shape[0]\n",
        "        recall = correct_predictions_count / total_actual_count\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length, precision, recall, BLEU, perplexity, BERT_P, BERT_R, BERT_F1"
      ],
      "metadata": {
        "id": "SquSfMg0c95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07c4311-543c-4da4-8dd0-48851459b2f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    print_precision_total = 0  # Reset every plot_every\n",
        "    print_recall_total = 0  # Reset every plot_every\n",
        "    print_BLEU_total = 0  # Reset every plot_every\n",
        "    print_perplexity_total = 0 # Reset every plot_every\n",
        "    print_BERT_P_total = 0\n",
        "    print_BERT_R_total = 0\n",
        "    print_BERT_F1_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss, precision, recall, BLEU, perplexity, BERT_P, BERT_R, BERT_F1 = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "        print_precision_total += precision\n",
        "        print_recall_total += recall\n",
        "        print_BLEU_total += BLEU\n",
        "        print_perplexity_total += perplexity\n",
        "        print_BERT_P_total += BERT_P\n",
        "        print_BERT_R_total += BERT_R\n",
        "        print_BERT_F1_total += BERT_F1\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_precision_avg = print_precision_total / print_every\n",
        "            print_recall_avg = print_recall_total / print_every\n",
        "            print_BLEU_avg = print_BLEU_total / print_every\n",
        "            print_perplexity_avg = print_perplexity_total / print_every\n",
        "            print_BERT_P_avg = print_BERT_P_total / print_every\n",
        "            print_BERT_R_avg = print_BERT_R_total / print_every\n",
        "            print_BERT_F1_avg = print_BERT_F1_total / print_every\n",
        "\n",
        "            print_loss_total = 0\n",
        "            print_precision_total = 0\n",
        "            print_recall_total = 0\n",
        "            print_BLEU_total = 0\n",
        "            print_perplexity_total = 0\n",
        "            print_BERT_P_total, print_BERT_R_total, print_BERT_F1_total = 0, 0, 0\n",
        "\n",
        "            print('%s (%d %d%%) %.4f  Precision: %.4f  Recall: %.4f  F1: %.4f, BLEU: %.4f, Perplexity: %.4f, BERT Scores (Prec, Rec, F1): %.4f, %.4f, %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg, print_precision_avg, print_recall_avg,\n",
        "                                         ((2*print_precision_avg*print_recall_avg)/(print_recall_avg+print_precision_avg)), print_BLEU_avg, \n",
        "                                         print_perplexity_avg, print_BERT_P_avg, print_BERT_R_avg, print_BERT_F1_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "VJnSm6OkdEgU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(question_details.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, answer_details.n_words, dropout_p=0.1).to(device)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Order of Performance Matrics - Precision, Recall, F1, BLEU, Perplexity, BERT Precision, BERT Recall, BERT F1 Score\n",
        "trainIters(encoder1, attn_decoder1, 50000, print_every=5000)\n",
        "# trainIters(encoder1, attn_decoder1, 50, print_every=5)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iltrDacLdowW",
        "outputId": "6dcf5b35-fe26-4957-99c2-9801c30a8f06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m 46s (- 24m 55s) (5000 10%) 1.7882  Precision: 0.3778  Recall: 0.3467  F1: 0.3616, BLEU: 0.3402, Perplexity: 3.9060, BERT Scores (Prec, Rec, F1): 0.5434, 0.3733, 0.4542\n",
            "5m 24s (- 21m 38s) (10000 20%) 1.3172  Precision: 0.5728  Recall: 0.5132  F1: 0.5414, BLEU: 0.5054, Perplexity: 2.2722, BERT Scores (Prec, Rec, F1): 0.5778, 0.3966, 0.4824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8m 29s (- 19m 48s) (15000 30%) 1.1311  Precision: 0.6515  Recall: 0.5849  F1: 0.6164, BLEU: 0.5801, Perplexity: 2.3787, BERT Scores (Prec, Rec, F1): 0.5920, 0.4563, 0.5204\n",
            "11m 19s (- 16m 58s) (20000 40%) 0.9243  Precision: 0.7239  Recall: 0.6544  F1: 0.6874, BLEU: 0.6473, Perplexity: 2.1187, BERT Scores (Prec, Rec, F1): 0.6388, 0.5396, 0.5863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14m 22s (- 14m 22s) (25000 50%) 0.7879  Precision: 0.7875  Recall: 0.7182  F1: 0.7512, BLEU: 0.7107, Perplexity: 1.8969, BERT Scores (Prec, Rec, F1): 0.6778, 0.6067, 0.6400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17m 13s (- 11m 29s) (30000 60%) 0.6602  Precision: 0.8275  Recall: 0.7733  F1: 0.7995, BLEU: 0.7650, Perplexity: 1.7122, BERT Scores (Prec, Rec, F1): 0.7304, 0.6809, 0.7041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20m 20s (- 8m 42s) (35000 70%) 0.5593  Precision: 0.8363  Recall: 0.7902  F1: 0.8126, BLEU: 0.7818, Perplexity: 1.5543, BERT Scores (Prec, Rec, F1): 0.7597, 0.7147, 0.7358\n",
            "23m 19s (- 5m 49s) (40000 80%) 0.4803  Precision: 0.8474  Recall: 0.8109  F1: 0.8287, BLEU: 0.7999, Perplexity: 1.4447, BERT Scores (Prec, Rec, F1): 0.7782, 0.7444, 0.7600\n",
            "25m 58s (- 2m 53s) (45000 90%) 0.4031  Precision: 0.8638  Recall: 0.8347  F1: 0.8490, BLEU: 0.8220, Perplexity: 1.3385, BERT Scores (Prec, Rec, F1): 0.8039, 0.7739, 0.7876\n",
            "28m 41s (- 0m 0s) (50000 100%) 0.3794  Precision: 0.8640  Recall: 0.8346  F1: 0.8490, BLEU: 0.8216, Perplexity: 1.2909, BERT Scores (Prec, Rec, F1): 0.8090, 0.7819, 0.7942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the epochs for all the KPI's.\n",
        "\n",
        "1.   Prec, Recall, F1 Score - Precision identifies the ratio of correctly predicted words out of all the predicted words sentence. Recall identifes the ratio of correctly predicted words out of all the words in reference sentence. F1 Score is a kind of trade-off between Precision and Recall. All these 3 measures kept on increasing till last epoch. They seems to be settling down in last e2 epochs. High value means better prediction.\n",
        "2.   BLEU Score - BLEU score is a kind of precision matrix for machine translation or related NLG tasks. We calculate unigram and bigram (ngrams) precision and then take a geometric average of both these precision values. The last step is to penalize a predicted sentence with smaller length than reference sentence. The higher BLUE score, the better is output. This KPI is also increasing in all epochs which is a good indication that we are learning in every iteration and improving.\n",
        "3.   Perplexity is a measure of reciprocal of probablity (chain rule) of the predicted sentence. This measure is then normalized by number of words. The lower value of perplexity is better. Perplexity is reducing with every epoch which is a good indication that we are making more concrete predictions.\n",
        "4.   BERT Precision, BERT Recall, BERT F1 - BERT score converts predicted and reference sentence into numerical representation based on word embeddings. The embeddings are then used to compute similarity in predicted and reference sentence. These KPI's are giving similar behaviour as seen in point 1 which is a good indication as wel.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YeaA1fa50uGO"
      }
    }
  ]
}